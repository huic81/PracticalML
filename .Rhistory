data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coefficient
summary(fit)$coefficient[1,1] + summary(fit)$coefficient[3,1]
summary(fit)$coefficient[3,1]
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coefficient
data(mtcars)
fit <- lm(mpg ~ factor(cyl), data = mtcars)
summary(fit)$coefficient
summary(fit)$coefficient[3,1]
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coefficient
summary(fit)$coefficient[3,1]
fit1 <- lm(mpg ~ factor(cyl), data = mtcars)
summary(fit1)$coefficient
summary(fit1)$coefficient[3,1]
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl)*wt, data = mtcars)
summary(fit1)$coefficient
summary(fit2)$coefficient
anova(fit1, fit2, test = "Chisq")
fit <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit)$coefficient
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit)
1000*0.5
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
dfbetas(fit)
dfbetas(fit)[5, 2]
CompanyDF <- read.csv("company_data.csv",header=TRUE)
dim(CompanyDF)
names(CompanyDF)
library(ggplot2)
g <- ggplot(CompanyDF, aes(x=x1,y=y)) + geom_point() + ggtitle("Study of influence of x1 on y")
g <- g + geom_smooth(method = "lm", colour = "blue")
g
fit <- lm(y~x1,CompanyDF)
summary(fit)
url <- "https://d3c33hcgiwev3.cloudfront.net/_cf0fd3361e05f5be5304b07b771bad48_company_data.csv?Expires=1488758400&Signature=MFkvz880anADj1ezMS7PGPHG1U~1z9IfefnJItor3s3tcumFf0YVsUa5wYBcqB-FsX50CdBswxYpt2oiySk~uo18xc0umZ0R79Y~kSCBuKwY-DNGYcKhrwAZNifPLk-ebxIrCSqOiVW9cndyR-0XOog8RtuQBG1gGp8xjKEFqCw_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A"
download.file(url,"company_data.csv")
CompanyDF <- read.csv("company_data.csv",header=TRUE)
dim(CompanyDF)
names(CompanyDF)
library(ggplot2)
g <- ggplot(CompanyDF, aes(x=x1,y=y)) + geom_point() + ggtitle("Study of influence of x1 on y")
g <- g + geom_smooth(method = "lm", colour = "blue")
g
fit <- lm(y~x1,CompanyDF)
summary(fit)
install.packages("ggplot2")
library(ggplot2)
g <- ggplot(CompanyDF, aes(x=x1,y=y)) + geom_point() + ggtitle("Study of influence of x1 on y")
g <- g + geom_smooth(method = "lm", colour = "blue")
g
fit <- lm(y~x1,CompanyDF)
summary(fit)
summary(fit)$coefficients[2,1]
coefs[2,1]+qt(0.975,fit$df-2)*c(-1,1)*coefs[2,2]
coefs <- summary(fit)$coefficients[2,1]
coefs
#Q3
CongInt <- coefs[2,1]+qt(0.975,fit$df-2)*c(-1,1)*coefs[2,2]
CongInt
coefs <- summary(fit)$coefficients
coefs[2,1]
#Q3
CongInt <- coefs[2,1]+qt(0.975,fit$df-2)*c(-1,1)*coefs[2,2]
CongInt
coefs[2,4]
#Q1
#Download source
url <- "https://d3c33hcgiwev3.cloudfront.net/_cf0fd3361e05f5be5304b07b771bad48_company_data.csv?Expires=1488758400&Signature=MFkvz880anADj1ezMS7PGPHG1U~1z9IfefnJItor3s3tcumFf0YVsUa5wYBcqB-FsX50CdBswxYpt2oiySk~uo18xc0umZ0R79Y~kSCBuKwY-DNGYcKhrwAZNifPLk-ebxIrCSqOiVW9cndyR-0XOog8RtuQBG1gGp8xjKEFqCw_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A"
download.file(url,"company_data.csv")
CompanyDF <- read.csv("company_data.csv",header=TRUE)
#Explore the data
dim(CompanyDF)
names(CompanyDF)
#Explore the data - check the plot
install.packages("ggplot2")
library(ggplot2)
g <- ggplot(CompanyDF, aes(x=x1,y=y)) + geom_point() + ggtitle("Study of influence of x1 on y")
g <- g + geom_smooth(method = "lm", colour = "blue")
g
#the plot seems to fit into linear regression line
fit <- lm(y~x1,CompanyDF)
summary(fit)
#value of R^2 is 0.7908, which means the ratio of the total variation explaned by the model is high.
#means x1 can explain most of the value of y.
#Q2
#Coefficient for x1 is at row 2 under Estimate
coefs <- summary(fit)$coefficients
coefs[2,1]
#Q3
#95% confidence interval is built on the value of the estimate and the standard error of the slope
CongInt <- coefs[2,1]+qt(0.975,fit$df-2)*c(-1,1)*coefs[2,2]
CongInt
#Q4
#pValue associated with the coefficient for x1 is at row 2 under Pr(>|t|)
coefs[2,4]
install.packages("ggplot2")
url <- "https://d3c33hcgiwev3.cloudfront.net/_cf0fd3361e05f5be5304b07b771bad48_company_data.csv?Expires=1488758400&Signature=MFkvz880anADj1ezMS7PGPHG1U~1z9IfefnJItor3s3tcumFf0YVsUa5wYBcqB-FsX50CdBswxYpt2oiySk~uo18xc0umZ0R79Y~kSCBuKwY-DNGYcKhrwAZNifPLk-ebxIrCSqOiVW9cndyR-0XOog8RtuQBG1gGp8xjKEFqCw_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A"
download.file(url,"company_data.csv")
CompanyDF <- read.csv("company_data.csv",header=TRUE)
#Explore the data
dim(CompanyDF)
names(CompanyDF)
g <- ggplot(CompanyDF, aes(x=x1,y=y)) + geom_point() + ggtitle("Study of influence of x1 on y")
g <- g + geom_smooth(method = "lm", colour = "blue")
g
#the plot seems to fit into linear regression line
fit <- lm(y~x1,CompanyDF)
summary(fit)
#value of R^2 is 0.7908, which means the ratio of the total variation explaned by the model is high.
#means x1 can explain most of the value of y.
#Q2
#Coefficient for x1 is at row 2 under Estimate
coefs <- summary(fit)$coefficients
coefs[2,1]
#Q3
#95% confidence interval is built on the value of the estimate and the standard error of the slope
CongInt <- coefs[2,1]+qt(0.975,fit$df-2)*c(-1,1)*coefs[2,2]
CongInt
#Q4
#pValue associated with the coefficient for x1 is at row 2 under Pr(>|t|)
coefs[2,4]
install.packages("Swirl")
install.packages("swirl")
packageVersion("swirl")
library(swirl)
install_from_swirl("Regression Models")
switl()
swirl()
rgp1()
rgp2()
head(swiss)
bdl <- lm(Fertility ~ ., swiss)
mdl <- lm(Fertility ~ ., swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . - Examination, swiss)
vif(mdl2)
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lowwer.tail=FALSE)
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
revenData
ravenData
glm(ravenWinNum ~ ravenScore,family="binomial", ravenData)
mdl <- glm(ravenWinNum ~ ravenScore,family="binomial", ravenData)
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
View(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'vivits'])
which.max(hits[,'visits'])
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
mdl2 <- glm(visits ~ date, poisson, offset=log(visits+1), hits)
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits,
| offset = log(visits + 1))
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
library(MASS)
shuttle$auto <- as.integer(shuttle$use == "auto")
fit <- glm(auto ~ wind - 1 , "binomial", shuttle)
cf <- exp(coef(fit))
oddsratio <- cf[1]/cf[2]
oddsratio
library(MASS)
data("shuttle")
summary(shuttle)
head(shuttle)
shuttle$use2<-as.integer(shuttle$use=="auto")
shuttle$wind2<-as.integer(shuttle$wind=="head")
logRegShuttle<-glm(use2~wind2, family=binomial, shuttle)
plot(x=shuttle$wind2, logRegShuttle$fitted, pch=19, col="blue", xlab="Wind", ylab="Use (auto)")
summary(logRegShuttle)$coef
exp(coef(logRegShuttle))
library(MASS)
shuttle$auto <- as.integer(shuttle$use == "auto")
fit <- glm(auto ~ wind - 1 , "binomial", shuttle)
cf <- exp(coef(fit))
oddsratio <- cf[1]/cf[2]
oddsratio
library(MASS)
shuttle$auto <- as.integer(shuttle$use == "auto")
fit <- glm(auto ~ wind + magn - 1 , "binomial", shuttle)
cf <- exp(coef(fit))
oddsratio <- cf[1]/cf[2]
oddsratio
fit3 <- glm(I(1 - auto) ~ wind - 1 , "binomial", shuttle)
rbind(coef(fit),coef(fit3))
logRegShuttle3<-glm(use2~wind, family=binomial, shuttle)
summary(logRegShuttle3)$coef
logRegShuttle32<-glm(1-use2~wind, family=binomial, shuttle)
summary(logRegShuttle32)$coef
data("InsectSprays")
head(InsectSprays)
fit4 <- glm(count ~ spray -1, poisson, InsectSprays)
cf4 <- exp(coef(fit4))
cf4[1]/cf4[2]
logRegInsect51<-glm(count~factor(spray)+offset(log(count+1)), family="poisson", InsectSprays)
summary(logRegInsect51)$coef
logRegInsect52<-glm(count~factor(spray)+offset(log(count+1)+log(10)), family="poisson", InsectSprays)
summary(logRegInsect52)$coef
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots <- 0
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
(xMat <- cbind(1, x, splineTerms))
(fit6 <- lm(y ~ xMat - 1))
yhat <- predict(fit6)
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit6$coef[2] + fit6$coef[3]
spline_term = x*(x>0)
regr = cbind(1,x,spline_term)
fit = lm(y~regr-1)
yhat = predict(fit)
plot(x,y,frame=FALSE,pch=21,bg='lightblue',cex=2)
lines(x,yhat,col='red',lwd=2)
ans=(yhat[11]-yhat[7])/4
ans
?cars
?mtcars
data("mtcars")
# Check the dataset
str(mtcars)
head(mtcars,6)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IdxCol_IL <- grep("^IL", names(training))
train_IL <- training[,IdxCol_IL]
test_IL <- testing[,IdxCol_IL]
preproc <- preProcess(train_IL, method="pca", thresh=0.9)
preproc$numComp
IdxCol_IL <- grep("^IL", names(training))
train_IL <- training[,IdxCol_IL]
test_IL <- testing[,IdxCol_IL]
preproc <- preProcess(train_IL, method="pca", thresh=0.8)
preproc$numComp
IdxCol_IL <- grep("^IL", names(training))
train_IL <- training[,IdxCol_IL]
test_IL <- testing[,IdxCol_IL]
preproc <- preProcess(train_IL, method="pca", thresh=0.9)
preproc$numComp
install.packages("AppliedPredictiveModeling")
install.packages("caret")
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
# 3.
modFit$finalModel
install.packages("e1071")
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
# 3.
modFit$finalModel
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
# Question 1
library(AppliedPredictiveModeling)
library(caret)
data(segmentationOriginal)
set.seed(125)
#inTrain <- createDataPartition(segmentationOriginal$Case, list=FALSE)
inTrain <- data$Case == "Train"
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
fit <- train(Class ~ ., data=training, method='rpart')
# Show decision tree: PS, WS, PS, Not possible to predict.
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
fit <- train(Class ~ ., data=training, method='rpart')
# Show decision tree: PS, WS, PS, Not possible to predict.
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
#modFit <- train(Class ~ ., method = "rpart", data = training)
# 3.
#modFit$finalModel
fit <- train(Class ~ ., data=training, method='rpart')
# Show decision tree: PS, WS, PS, Not possible to predict.
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
fit$finalModel
inTrain <- data$Case == "Train"
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
#modFit <- train(Class ~ ., method = "rpart", data = training)
# 3.
#modFit$finalModel
fit <- train(Class ~ ., data=training, method='rpart')
# Show decision tree: PS, WS, PS, Not possible to predict.
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
fit$finalModel
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
#modFit <- train(Class ~ ., method = "rpart", data = training)
# 3.
#modFit$finalModel
fit <- train(Class ~ ., data=training, method='rpart')
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
fit <- train(Class ~ ., data=training, method='rpart')
# Show decision tree: PS, WS, PS, Not possible to predict.
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
fit$finalModel
training<-segmentationOriginal[segmentationOriginal$Case=="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
model<-train(Class ~ .,
data = training,
method = "rpart")
model$finalModel
fancyRpartPlot(model$finalModel)
library(rpart.plot)
fancyRpartPlot(model$finalModel)
installed.packages("rpart.plot")
fancyRpartPlot(model$finalModel)
library(rpart.plot)
fancyRpartPlot(model$finalModel)
subset <- split(segmentationOriginal, segmentationOriginal$Case)
set.seed(125)
modCART <- rpart(Class ~ ., data=subset$Train)
modCART$finalModel
subset <- split(segmentationOriginal, segmentationOriginal$Case)
set.seed(125)
modCART <- rpart(Class ~ ., data=subset$Train)
modCART
plot(modCART, uniform=T)
text(modCART, cex=0.8)
View(train)
testA <- segmentationOriginal[0,]
testA[1,c("TotalIntenCh2", "FiberWidthCh1", "PerimStatusCh1")] <- c(23000, 10, 2)
predict(modCART, testA, type="prob")
# b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100
testB <- segmentationOriginal[0,]
testB[1,c("TotalIntenCh2", "FiberWidthCh1", "VarIntenCh4")] <- c(50000, 10, 100)
predict(modCART, testB, type="prob")
# c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100
testC <- segmentationOriginal[0,]
testC[1,c("TotalIntenCh2", "FiberWidthCh1", "VarIntenCh4")] <- c(57000, 8, 100)
predict(modCART, testC, type="prob")
# d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
testD <- segmentationOriginal[0,]
testD[1,c("FiberWidthCh1", "VarIntenCh4","PerimStatusCh1")] <- c(8, 100, 2)
predict(modCART, testD, type="prob")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
newdata
set.seed(125)
modCART2 <- rpart(Area ~ ., data=olive)
modCART2
predict(modCART2, newdata, type="tree")
predict(modCART2, newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial")
logitModel
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
# Training Set Misclassification rate
missClass(trainSA$chd, predictTrain) # 0.2727273
# Test Set Misclassification rate
missClass(testSA$chd, predictTest) # 0.3116883
modolive <- train(Area ~ ., method = "rpart", data = olive)
predict(modolive, newdata = newdata)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
modolive <- train(Area ~ ., method = "rpart", data = olive)
predict(modolive, newdata = newdata)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modRF <- train(y ~ ., data=vowel.train, method="rf")
modRF <- train(y ~ ., data=vowel.train, method="rf")
res <- predict(modRF,vowel.test)
varImp(modRF)
data("mtcars")
boxplot(mpg ~ am, data = mtcars, col = (c("blue","green")), ylab = "Miles Per (US) Gallon", xlab = "Transmission Type", names = c("Automatic", "Manual"), main="Box plot of MPG vs. Transmission")
View(mtcars)
View(mtcars)
View(mtcars)
View(mtcars)
cols <- character(nrow(mtcars))
cols[] <- "black"
cols[mtcars$am == 0] <- "blue"
cols[mtcars$am == 1] <- "green"
pairs(mpg ~ ., panel = panel.smooth, col = cols, data = mtcars, main="Pairs Plot of Motor Trend Car Road Tests")
bettermodel <- lm(mpg ~ wt + qsec + am + wt:am, data = mtcars)
par(mfrow = c(2, 2))
plot(bettermodel)
# Input load. Please do not change #
`dataset` = read.csv('C:/Users/hwee.see.teh/AppData/Local/Radio/REditorWrapper_56247b90-1e04-4f46-825a-0d3ff147f235/input_df_83c6b2cb-7655-4f3e-a948-cfd44782ac64.csv', check.names = FALSE, encoding = "UTF-8", blank.lines.skip = FALSE);
# Original Script. Please update your script content here and once completed copy below section back to the original editing window #
library(ggplot2)
pairs(mpg ~ ., panel = panel.smooth, col = am, data = dataset, main="Pairs Plot of Motor Trend Car Road Tests")
setwd("~/")
setwd("~/Git/PracticalML/PracticalML")
setwd("~/Git/PracticalML/PracticalML")
